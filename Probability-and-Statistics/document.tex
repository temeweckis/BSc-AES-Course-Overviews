% Template setup and packages.
\documentclass[10pt,landscape,a4paper]{article}
\usepackage{multicol}
\usepackage{calc}
\usepackage{ifthen}
\usepackage{geometry}

% Custom packages.
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{stix2}
\usepackage{systeme}
\usepackage{graphicx}
\usepackage{float}
\usepackage{physics}
\usepackage{siunitx}
\usepackage{collcell} % loads array
\newcolumntype{M}{>{$} l <{$}}
\newcolumntype{U}{>{$[\collectcell\si} l <{\endcollectcell]$}}

% Hyperref. Remember to change title!
\usepackage{hyperref}
\hypersetup{pdfauthor={Teemu Weckroth},pdftitle={Probability \& Statistics}}

% Path to graphics folder.
\graphicspath{ {./figures/} }

% Change fonts for v and w.
\DeclareSymbolFont{txletters}{OML}{ntxmi}{m}{it}
\SetSymbolFont{txletters}{bold}{OML}{ntxmi}{b}{it}
\DeclareFontSubstitution{OML}{ntxmi}{m}{it}
\DeclareMathSymbol{v}{\mathalpha}{txletters}{`v}
\DeclareMathSymbol{w}{\mathalpha}{txletters}{`w}

% Must be below font changes to avoid errors.
\usepackage{bm}

% Commands for differentials. Redefines the underdot command!
\renewcommand\d{\mathop{}\!\mathrm{d}}
\newcommand\p{\mathop{}\!\mathrm{\partial}}
%\newcommand\e{\mathrm{e}}

% Commands for the set of real numbers and Lagrangian/Laplace.
\newcommand{\R}{\mathbb{R}}
\newcommand{\La}{\mathscr{L}}

% Unbreakable unit environment.
\newenvironment{absolutelynopagebreak}
{\par\nobreak\vfil\penalty0\vfilneg
	\vtop\bgroup}
{\par\xdef\tpd{\the\prevdepth}\egroup
	\prevdepth=\tpd}

% Shrink bullet points.
\renewcommand\labelitemi{$\vcenter{\hbox{\tiny$\bullet$}}$}

%
\ifthenelse{\lengthtest { \paperwidth = 11in}}
{ \geometry{top=.5in,left=.5in,right=.5in,bottom=.5in} }
{\ifthenelse{ \lengthtest{ \paperwidth = 297mm}}
	{\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
	{\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
}

% Turn off header and footer
\pagestyle{empty}

% Redefine section commands to use less space
\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}%
	{-1ex plus -.5ex minus -.2ex}%
	{0.5ex plus .2ex}%x
	{\normalfont\large\bfseries}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}%
	{-1explus -.5ex minus -.2ex}%
	{0.5ex plus .2ex}%
	{\normalfont\normalsize\bfseries}}
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{0mm}%
	{-1ex plus -.5ex minus -.2ex}%
	{1ex plus .2ex}%
	{\normalfont\small\bfseries}}
\makeatother

% Define BibTeX command
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
		T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

% Don't print section numbers
\setcounter{secnumdepth}{0}

\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt plus 0.5ex}

\begin{document}
	
	\raggedright
	\footnotesize
	\begin{multicols}{3}
		% multicol parameters
		% These lengths are set only within the two main columns
		%\setlength{\columnseprule}{0.25pt}
		\setlength{\premulticols}{1pt}
		\setlength{\postmulticols}{1pt}
		\setlength{\multicolsep}{1pt}
		\setlength{\columnsep}{2pt}
		
		\part*{Probability \& Statistics}
		\begin{center}
			Teemu Weckroth, \today \\
		\end{center}
		
		\section{De Morgan's Laws}
		\begin{equation*}
			(A \cup B)^C = A^C \cap B^C
		\end{equation*}
		\begin{equation*}
			(A \cap B)^C = A^C \cup B^C
		\end{equation*}
		
		\section{Probability function}
		Let $\Omega$ be a finite sample space. A probability function $P$ assigns to each
		event $A$ in $\Omega$ a number $P(A)$ in $[0,1]$ such that:
		\begin{enumerate}
			\item $P(\Omega) = 1$
			\item $P(A \cup B) = P(A) + P(B)$ if $A$ and $B$ are disjoint
		\end{enumerate}
		The number $P(A)$ is called the probability of $A$
		
		\section{Addition and complement rule}
		For any two events $A$ and $B$ we have
		\begin{equation*}
			P(A \cup B) = P(A) + P(B) - P(A \cap B)
		\end{equation*}
		For any event $A$ we have
		\begin{equation*}
			P(A^C)=1 - P(A)
		\end{equation*}
		
		\section{Conditional probability}
		The conditional probability of $A$ given $C$ is defined as
		\begin{equation*}
			P(A \mid C) = \frac{P(A \cap C)}{P(C)},
		\end{equation*}
		provided that $P(C) > 0$
		
		\section{Multiplication rule}
		For any events $A$ and $C$ it hold that
		\begin{equation*}
			P(A \cap C) = P(A \mid C) P(C)
		\end{equation*}
		
		\section{Independence equivalence}
		\begin{equation*}
			P(B \mid A) = \frac{P(A \cap B)}{P(A)}
						= \frac{P(A \mid B) P(B)}{P(A)}
						= \frac{P(A) P(B)}{P(A)}
						= P(B)
		\end{equation*}
		
		\section{Law of total probability}
		Let $A$ and $C$ be two events. We have
		\begin{equation*}
			P(A) = P(A \mid C) P(C) + P(A \mid C^C) P(C^C)
		\end{equation*}
		Suppose we have disjoint events $C_1, C_2,...,C_m$ such that
		$C_1 \cup C_2 \cup ... \cup C_m = \Omega$. For any event $A$ we have
		\begin{equation*}
			P(A)=P(A \mid C_1) P(C_1) + P(A \mid C_2) P(C_2) + ... + P(A \mid C_m) P(C_m)
		\end{equation*}
		
		\section{Bayes' rule}
		Suppose the events $C_1, C_2, ..., C_m$ are disjoint and fill up the
		sample space $\Omega$. Then the conditional probability of $C_i$ given
		the same event A is
		\begin{equation*}
			P(C_i \mid A) = \frac{P(A \mid C_i) P(C_i)}{P(A \mid C_1)P(C_1)
							+ ... + P(A \mid C_m)P(C_m)}
		\end{equation*}
		
		\section{Discrete random variable}
		Let $\Omega$ be a sample space. A discrete random variable is a function
		$X: \Omega \rightarrow \mathbb{R}$ that takes on a finite number of values
		$a_1, a_2, ..., a_n$ or an infinite number of values $a_1, a_2, a_3, ...$
		
		\section{Probability mass function}
		The probability mass function $p$ of a discrete random variable $X$ is the
		function $p: \mathbb{R} \rightarrow [0,1]$ defined by
		\begin{equation*}
			p(a) = P(X=a) \textrm{ for $-\infty < a < \infty$}
		\end{equation*}
		
		\section{Distribution function}
		The distribution function $F$ of a discrete random variable $X$ is the function
		$F: \mathbb{R} \rightarrow [0,1]$ defined by
		\begin{equation*}
			F(a) = P(X \leq a) \textrm{for $-\infty < a < \infty$}
		\end{equation*}
		
		\section{Binomial coefficient}
		The binomial coefficient ${n \choose k}$ gives the number of combinations of $k$
		objects from a set of $n$ objects:
		\begin{equation*}
			{n \choose k} = \frac{n!}{k! (n-k)!}
		\end{equation*}
		
		\section{Bernoulli distribution}
		A random variable $X$ has a Bernoulli distribution if $X$ only takes on
		the values $0$ and $1$ with probabilities
		\begin{equation*}
			P(X = 1) = p
		\end{equation*}
		\begin{equation*}
			P(X = 0) = 1 - p
		\end{equation*}
		$X \sim \mathrm{Ber}(p)$
		
		\section{Binomial distribution}
		A random variable $X$ has a binomial distribution with parameters $n$ and $p$
		if $X$ can take on the values $k = 0,1,...,n$ with probabilities
		\begin{equation*}
			P(X = k) = {n \choose k} p^k (1-p)^{n-k}
		\end{equation*}
		$X \sim \mathrm{Bin}(n,p)$
		
		\section{Geometric distribution}
		A random variable $X$ has a geometric distribution with parameter $p$ if $X$
		can take on the values $k = 1,2,3,...$ with probabilities
		\begin{equation*}
			P(X = k) = p \cdot (1-p)^{k-1}
		\end{equation*}
		$X \sim \mathrm{Geo}(p)$
		
		\section{Poisson distribution}
		A random variable $X$ has a Poisson distribution with parameter $\mu$ if $X$
		can take on the values $k = 0,1,2,...$ with probabilities
		\begin{equation*}
			P(X = k) = \frac{\mu^k}{k!} e^{-\mu}
		\end{equation*}
		$X \sim \mathrm{Pois}(\mu)$ \\
		$Y \approx \mathrm{Pois}(np) \textrm{ for } Y \sim \mathrm{Bin}(n,p) \textrm{ with large $n$ and small $np$}$
		
		\section{Uniform distribution}
		A continuous random variable $X$ has a uniform distribution on the interval
		$[\alpha, \beta]$ if its probability density function $f$ is given by
		\begin{equation*}
			f(x) =	\begin{cases}
					\frac{1}{\beta - \alpha} & \mbox{for } x \in [\alpha, \beta] \\
					0 & \mbox{for } x \notin [\alpha, \beta]
					\end{cases}
		\end{equation*}
		$X \sim \mathrm{U}(\alpha, \beta)$
		
		\section{Exponential distribution}
		A continuous random variable $X$ has an exponential distribution with parameter
		$\lambda$ if its probability density function $f$ is given by
		\begin{equation*}
			f(x) =	\begin{cases}
					\lambda e^{- \lambda x} & \mbox{for } x \geq 0 \\
					0 & \mbox{for } x < 0
					\end{cases}
		\end{equation*}
		$X \sim \mathrm{Exp}(\lambda)$
		
		\section{Pareto distribution}
		A continuous random variable $X$ has a pareto distribution with parameter $a > 0$
		if its probability density function $f$ is given by
		\begin{equation*}
			f(x) =	\begin{cases}
					\frac{\alpha}{x^{\alpha + 1}} & \mbox{for } x \geq 1 \\
					0 & \mbox{for } x < 1
					\end{cases}
		\end{equation*}
		$X \sim \mathrm{Par}(\alpha)$
		
		\section{Normal distribution}
		A continuous random variable $X$ has a normal distribution with parameters $\mu$
		and $\sigma^2$ if its probability density function $f$ is given by
		\begin{equation*}
			f(x) =	\frac{1}{\sigma \sqrt{2 \pi}} e^{- \frac{1}{2} (\frac{x - \mu}{\sigma})^2}
		\end{equation*}
		$X \sim \mathrm{N}(\mu, \sigma^2)$
		
		\section{Standard normal distribution}
		If $\mu = 0$ and $\sigma^2 = 1$, the distribution $\mathrm{N}(0,1)$ is called the standard
		normal distribution
		
		\section{Quantiles}
		Let $X$ be a continuous random variable and $0 \leq p \leq 1$. The $p$th quantile
		or the $100p$th percentile of the distribution of $X$ is the smallest number
		$q_p$ such that
		\begin{equation*}
			F_X(q_p) =	P(X \leq q_p) = p
		\end{equation*}
		
		\section{Expectation of a random variable}
		Expectation $E[X]$ of a discrete random variable $X$ is defined as the number
		\begin{equation*}
			\mathrm{E}[X] =	\sum_{a_i} a_i \cdot P(X = a_i)
		\end{equation*}
		Expectation $E[X]$ of a continuous random variable $X$ with pdf $f$ is given by
		\begin{equation*}
			\mathrm{E}[X] =	\int_{-\infty}^{\infty} x \cdot f(x) dx
		\end{equation*}
		
		\section{Change-of-variable formula}
		Let $X$ be a RV and $g: \mathbb{R} \rightarrow \mathbb{R}$ a function \\
		If $X$ is discrete:
		\begin{equation*}
			\mathrm{E}[g(X)] =	\sum_i g(a_i) P(X = a_i)
		\end{equation*}
		If $X$ is continuous:
		\begin{equation*}
			\mathrm{E}[g(X)] = \int_{-\infty}^{\infty} g(x) f(x) dx
		\end{equation*}
		
		\section{Variance}
		Variance of a random variable $X$ is defined as
		\begin{equation*}
			\mathrm{Var}(X)	= \mathrm{E}[(X - E[X])^2]
					= \mathrm{E}[X^2] - (\mathrm{E}[X])^2
		\end{equation*}
		Standard deviation of a random variable $X$ is defined as
		\begin{equation*}
			\mathrm{SD}(X)	= \sqrt{\mathrm{Var}(X)}
		\end{equation*}
		
		\section{Change-of-units formula}
		For any random variable $X$ and any real values $r$ and $s$ it holds that
		\begin{equation*}
			\mathrm{E}[rX + s] = r\mathrm{E}[X] + s
		\end{equation*}
		\begin{equation*}
			\mathrm{Var}(rX + s) = r^2 \mathrm{Var}(X)
		\end{equation*}
		
		\section{Jensen's inequality}
		Let $g$ be a convex function and let $X$ be a random variable. Then
		\begin{equation*}
			g(\mathrm{E}[X]) \leq \mathrm{E}[g(x)]
		\end{equation*}
		Let $g$ be a concave function and let $X$ be a random variable. Then
		\begin{equation*}
			g(\mathrm{E}[X]) \geq \mathrm{E}[g(X)]
		\end{equation*}
		
		\section{Transforming normal RVs}
		Suppose $X \sim \mathrm{N}(\mu, \sigma^2)$, then the RV $rX + s$ also has a normal distribution:
		\begin{equation*}
			rX + s \sim \mathrm{N}(r \mu + s, r^2 \sigma^2)
		\end{equation*}
		Every normally distributed RV can also be transformed into a standard normal RV:
		\begin{equation*}
			\mbox{if } X \sim \mathrm{N}(\mu, \sigma^2), \mbox{ then }
			Z = \frac{X - \mu}{\sigma} \sim \mathrm{N}(0, 1)
		\end{equation*}
		
		\section{Joint probability mass function}
		Let $X$ and $Y$ be two discrete RVs.
		The joint probability mass function of $X$ and $Y$ is the function defined by
		$p: \mathbb{R}^2 \rightarrow [0,1]$
		\begin{equation*}
			p(a,b) = P(X = a, Y = b) \mbox{ for all $a$ and $b$}
		\end{equation*}
		From joint to marginal, take sum of rows and columns:
		\begin{equation*}
			p_X(x) = \sum_{y} p(x,y)
		\end{equation*}
		\begin{equation*}
			p_Y(y) = \sum_{x} p(x,y)
		\end{equation*}
		
		\section{Joint distribution function}
		Let $X$ and $Y$ be two RVs.
		The joint distribution function $F$ of $X$ and $Y$ is the function
		$F: \mathbb{R}^2 \rightarrow [0,1]$ defined by
		\begin{equation*}
			F(a,b) = P(X \leq a, Y \leq b) \mbox{ for all $a$ and $b$}
		\end{equation*}
		
		\section{Joint density function}
		Let $X$ and $Y$ be two continuous RVs.
		The joint density function $f$ of $X$ and $Y$ is the function
		$f: \mathbb{R}^2 \rightarrow \mathbb{R}$ such that
		\begin{equation*}
			P(a_1 \leq X \leq b_1, a_2 \leq Y \leq b_2) =
			\int_{a_2}^{b_2} \int_{a_1}^{b_1} f(x,y) dx dy
		\end{equation*}
		
		\section{Marginal density function}
		Let $f$ be the joint density function of $X$ and $Y$.
		Then the marginal densities of $X$ and $Y$ can be found as
		\begin{equation*}
			f_X(x) = \int_{-\infty}^{\infty} f(x,y) dy
		\end{equation*}
		and
		\begin{equation*}
			f_Y(y) = \int_{-\infty}^{\infty} f(x,y) dx
		\end{equation*}
		
		\section{Expectations of a function of two RVs}
		Let $X$ and $Y$ be random variables and let
		$g: \mathbb{R}^2 \rightarrow \mathbb{R}$ be a function \\
		If $X$ and $Y$ are discrete with values $a_1, a_2,...$ and $b_1, b_2,...$
		respectively, then
		\begin{equation*}
			E[g(X,Y)] = \sum_{i} \sum_{j} g(a_i,b_j) P(X=a_i, Y=b_j)
		\end{equation*}
		If $X$ and $Y$ are continuous with joint probability density function $f$, then
		\begin{equation*}
			E[g(X,Y)] = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} g(x,y) f(x,y) dx dy
		\end{equation*}
		
		\section{Covariance}
		Let $X$ and $Y$ be two RVs. The covariance between $X$ and $Y$ is
		\begin{equation*}
			\mathrm{Cov}(X,Y) = \mathrm{E}[(X - \mathrm{E}[X]) (Y - \mathrm{E}[Y])]
			= \mathrm{E}[XY] - \mathrm{E}[X] \mathrm{E}[Y]
		\end{equation*}
		If $\mathrm{Cov}(X,Y) > 0$, then $X$ and $Y$ are positive correlated \\
		If $\mathrm{Cov}(X,Y) < 0$, then $X$ and $Y$ are negatively correlated \\
		If $\mathrm{Cov}(X,Y) = 0$, then $X$ and $Y$ are not correlated \\
		Let $X$ and $Y$ be two RVs. Then always
		\begin{equation*}
			\mathrm{Var}(X+Y) = \mathrm{Var}(X) + \mathrm{Var}(Y) + 2\mathrm{Cov}(X,Y)
		\end{equation*}
		If $X$ and $Y$ are uncorrelated, then
		\begin{equation*}
			\mathrm{Var}(X+Y) = \mathrm{Var}(X) + \mathrm{Var}(Y)
		\end{equation*}
		Let $X$ and $Y$ be two RVs. Then
		\begin{equation*}
			\mathrm{Cov}(rX + s, tY + u) = rt \mathrm{Cov}(X,Y)
		\end{equation*}
		for all values $r$, $s$, $t$, and $u$
		
		\section{Correlation}
		Let $X$ and $Y$ be two RVs. The correlation coefficient is
		\begin{equation*}
			\rho (X,Y) = \frac{\mathrm{Cov} (X,Y)}{\sqrt{\mathrm{Var}(X)} \sqrt{\mathrm{Var}(Y)}}
		\end{equation*}
		if $\mathrm{Var}(X) > 0$ and $\mathrm{Var}(Y) > 0$. Else $\rho (X,Y) = 0$
		
		\section{Expected number of events}
		$\mathrm{E}[M(0,1)] = $ expected number of events in interval of unit length
		$ = $ intensity of the process $\lambda$
		\begin{equation*}
			\mathrm{E}[M(a,b)] = np = (b-a) \frac{n}{b-a} p = (b-a) \mathrm{E}[M(0,1)]
			= \lambda (b-a)
		\end{equation*}
		$N(a,b) \sim \mathrm{Pois}(\lambda(b-a))$
		
		\section{Sum of two independent discrete RVs}
		Let $X$ and $Y$ be two independent discrete RVs.
		The pmf of $Z = X + Y$ satisfies
		\begin{equation*}
			p_Z (c) = \sum_j p_X (c - b_j ) p_Y (b_j)
		\end{equation*}
		where the sum runs over all possible values $b_j$ of $Y$ \\
		If $X \sim \mathrm{Bin}(n,p)$, $Y \sim \mathrm{Bin}(m,p)$ and $X$ and $Y$ are
		independent, then $X + Y \sim \mathrm{Bin}(n + m, p)$
		
		\section{Sum of two independent continuous RVs}
		Let $X$ and $Y$ be two independent continuous RVs.
		The pdf of $Z = X + Y$ satisfies
		\begin{equation*}
			f_Z (z) = \int_{-\infty}^{\infty} f_X(z-y) f_Y(y) dy
		\end{equation*}
		If $X \sim \mathrm{N}(\mu, \sigma^2)$, $Y \sim \mathrm{N}(\nu, \tau^2)$ and
		$X$ and $Y$ are independent, then
		$X + Y \sim \mathrm{N}(\mu + \nu, \sigma^2 + \tau^2)$
		
		\section{Independent and identically distributed sequence of random events}
		$X_1, X_2, ..., X_n$ all have the same distribution and are independent \\
		Same expectations $\mathrm{E}[X_i] = \mu$ \\
		Same variances $\mathrm{Var}(X_i) = \sigma^2$
		\begin{equation*}
			\mathrm{E}[\bar{X}_n]
				= \mathrm{E} \big[ \frac{1}{n} \sum_{i=1}^{n} X_i \big]
				= \frac{1}{n} \sum_{i=1}^{n} \mathrm{E}[X_i]
				= \frac{1}{n} \sum_{i=1}^{n} \mu
				= \mu
		\end{equation*}
		
		\section{Rules of the variance}
		\begin{equation*}
			\mathrm{Var}(cX) = c^2 \mathrm{Var}(X)
		\end{equation*}
		For independent random variables:
		\begin{equation*}
			\mathrm{Var}(X + Y) = \mathrm{Var}(X) + \mathrm{Var}(Y)
		\end{equation*}
		
		\section{Variances of the average}
		If $X_1, X_2, ... ,X_n$ is an i.i.d. sequence, $\mathrm{Var}(X_i) = \sigma^2$, then
		\begin{equation*}
			\mathrm{Var}(\bar{X}_n) = \frac{\sigma^2}{n}
		\end{equation*}
		
		\section{Chebyshev's inequality}
		Suppose $\mathrm{E}(X) = \mu$, $\mathrm{Var}[X] = \sigma^2$. Then
		\begin{equation*}
			P(|X-\mu| \geq a) \leq \frac{\sigma^2}{a^2}
		\end{equation*}
		Taking $a=k\sigma$,
		\begin{equation*}
			P(|X-\mu| \geq k\sigma) \leq \frac{\sigma^2}{(k\sigma)^2} = \frac{1}{k^2}
		\end{equation*}
		
		\section{Law of large numbers}
		Let $X_1,...X_n$ be an i.i.d. sequence with $\mathrm{E}[X_i] = \mu$ and
		$\mathrm{Var}(X_i) = \sigma^2$. Then for any $\epsilon > 0$
		\begin{equation*}
			P(|\bar{X}_n - \mu| > \epsilon) \rightarrow 0 \mbox{ as } n \rightarrow \infty
		\end{equation*}
		
		\section{Central limit theorem}
		Let $X_1,...,X_n$ be an i.i.d. sequence with $\mathrm{E}[X_i] = \mu$ and
		$\mathrm{Var}(X_i) = \sigma^2 < \infty$. For $n \geq 1$, let $Z_n$ be defined by
		\begin{equation*}
			Z_n = \frac{X_n - \mu}{\sigma / \sqrt{n}}
		\end{equation*}
		Then for any number $a$ it holds that
		\begin{equation*}
			P(Z_n \leq a) \rightarrow P(Z \leq a) \mbox{ as } n \rightarrow \infty
		\end{equation*}
		where $Z$ has a $\mathrm{N}(0,1)$ distribution \\
		In other words: for large $n$, $Z_n$, $\bar{X}_n$, and $\sum X_i$ all approximately
		have a normal distribution
		\begin{equation*}
			Z_n \stackrel{d}{\approx} \mathrm{N}(0,1)
		\end{equation*}
		\begin{equation*}
			\bar{X}_n \stackrel{d}{\approx} \mathrm{N}(\mu, \sigma^2 / n)
		\end{equation*}
		\begin{equation*}
			\sum_{i=1}^{n} X_i \stackrel{d}{\approx} \mathrm{N}(n\mu, n\sigma^2)
		\end{equation*}
		
		\section{Histogram}
		\begin{enumerate}
			\item Divide the range of the data into intervals
			\item Determine the height
			\begin{equation*}
				\mbox{Height on } B_i = \frac{\#B_i}{n \cdot |B_i|}
			\end{equation*}
			\begin{equation*}
				\mbox{Area on } B_i = \frac{\#B_i}{n}
			\end{equation*}
		\end{enumerate}
		
		\section{Empirical distribution function}
		The value of the empirical distribution function at a point $x$ is equal to
		the fraction of datapoints that is smaller than or equal to $x$
		\begin{equation*}
			F_n(x) = \frac{\# x_i \leq x}{n}
		\end{equation*}
		
		\section{Standard deviation as a measure of variation}
		The sample variance of a dataset is defined as
		\begin{equation*}
			S_n^2 = \frac{1}{n-1} ( (x_1 - \bar{x}_n)^2 + ... + (x_n - \bar{x}_n)^2)
		\end{equation*}
		The sample standard deviation is defined as
		\begin{equation*}
			S_n = \sqrt{S_n^2}
		\end{equation*}
		
		\section{Median Absolute Deviation as measure of variation}
		The median of absolute deviation (MAD) of a dataset is defined as
		\begin{equation*}
			\mathrm{MAD} = \mathrm{Med}(|x_1 - m_n|,...,|x_n - m_n|)
		\end{equation*}
		
		\section{Five-number summary}
		\begin{enumerate}
			\item Minimum
			\item Lower quartile
			\item Median
			\item Upper quartile
			\item Maximum
		\end{enumerate}
		
		\section{Quartiles and their computation}
		Let $x_1,...,x_n$ be a dataset. For any $p \in [0,1]$ the $p$th empirical quantile is the number $q_n(p)$ such that a proportion $p$ of the dataset is less than $q_n(p)$ and a proportion $1-p$ is larger than $q_n(p)$ \\
		Let $x_1,...,x_n$ be an ordered dataset. Compute
		\begin{equation*}
			p(n+1) = k + \alpha,
		\end{equation*}
		where $k$ is the integer part of $p(n+1)$ and $a$ is its decimal part. Then
		\begin{equation*}
			q_n(p) = x_k + \alpha (x_{k+1} - x_k)
		\end{equation*}
		
		\section{Random sample and statistical model}
		A random sample is a collection of RVs $X_1, X_2, ..., X_n$ that have the same
		probability distribution and are mutually independent.
		
		\section{Model distribution}
		The probability distribution of each random variable from a random sample
		is called the model distribution \\
		The random variable $h(X_1,...,X_n)$ depending only on the random sample
		$X_1,...,X_n$ is called a sample statistics
		
		\section{Estimators}
		An estimate $t$ is a value that depends only on the data
		\begin{equation*}
			t=h(x_1,x_2,...,x_n)
		\end{equation*}
		An estimator is a random variable that gives the value of an estimate calculated
		from a random sample $X_1, X_2, ..., X_n$
		\begin{equation*}
			T=h(X_1,X_2,...,X_n)
		\end{equation*}
		
		\section{Unbiased estimators}
		An unbiased estimator is an estimator $T$ for the parameter $\lambda$ such that
		$\mathrm{E}[T] = \lambda$ for all values of $\lambda$
		
		\section{Sampling distribution}
		Let $T=h(X_1,X_2,...,X_n)$ be an estimator based on a random sample
		$X_1, X_2, ..., X_n$. The probability distribution of $T$ is called the
		sampling distribution of $T$.
		
		\section{Unbiased estimators for mean and variance}
		Let $X_1, X_2, ..., X_n$ be a random sample from a distribution with finite
		expectation $\mu$ and variance $\sigma^2$. Then
		\begin{equation*}
			\bar{X}_n = \frac{1}{n} \sum_{i=1}^{n} X_i
		\end{equation*}
		and
		\begin{equation*}
			S_n^2 = \frac{1}{n-1} \sum_{i=1}^{n} (X_i - \bar{X}_n)^2
		\end{equation*}
		are unbiased estimators of $\mu$ and $\sigma^2$
		
		\section{Mean squared error}
		Let $T$ be an estimator for the parameter $\theta$. The mean squared error
		of $T$ is defined as
		\begin{equation*}
			\mathrm{MSE}(T) =	\mathrm{E}[(T-\theta)^2] =
								\mathrm{Var}(T) + (\mathrm{E}[T]-\theta)^2
		\end{equation*}
		
		\section{Efficiency}
		Let $T_1$ and $T_2$ be two estimators for the same parameter. If
		\begin{equation*}
			\mathrm{MSE}(T_2) < \mathrm{MSE}(T_1),
		\end{equation*}
		we say that $T_2$ is more efficient than $T_1$
		
		\section{General coefficient intervals}
		Let $X_i$ have a distribution dependent on the parameter $\theta$. Suppose sample
		statistics $L_n=g(X_1,...,X_n)$ and $U_n=h(X_1,...,X_n)$ exist such that
		\begin{equation*}
			P(L_n < \theta < U_n) = \gamma
			\mbox{ for some } 0 < \gamma < 1 \mbox{ and all } \theta
		\end{equation*}
		Then, given a realization $x_1,...,x_n$ of the variables $X_1,...,X_n$ and
		$l_n=g(x_1,...,x_n)$ and $u_n=h(x_1,...,x_n)$, the interval
		\begin{equation*}
			(l_n,u_n)
		\end{equation*}
		is a $100 \gamma \%$ confidence interval for $\theta$
		
		\section{Critical values of the normal distribution}
		The critical value of a standard normal distribution is the real number
		$z_p$ such that
		\begin{equation*}
			P(Z \geq z_p) = p
		\end{equation*}
		where $Z \sim \mathrm{N}(0,1)$
		
		\section{Confidence interval for the mean of a normal distribution; variance known}
		Suppose $X_1,...,X_n$ are independent and normally distributed with parameters
		$\mu$ and $\sigma^2$. Then
		\begin{equation*}
			P(\bar{X}_n - z_{\frac{\alpha}{2}} \frac{\sigma}{\sqrt{n}}
			\leq \mu \leq
			\bar{X}_n + z_{\frac{\alpha}{2}} \frac{\sigma}{\sqrt{n}})
			= 1 - \alpha
		\end{equation*}
		
		\section{Confidence interval for the mean of a normal distribution; variance unknown}
		Suppose $X_1,...,X_n$ are independent and normally distributed with parameters
		$\mu$ and $\sigma^2$.
		If the dataset $x_1,...,x_n$ is a realization of the random sample $X_1,...,X_n$
		and $\gamma = 1-\alpha$, then
		\begin{equation*}
			\big(
			\bar{x}_n - t_{n-1, a/2} \frac{s_n}{\sqrt{n}},
			\bar{x}_n + t_{n-1, a/2} \frac{s_n}{\sqrt{n}}
			\big)
		\end{equation*}
		is called a $100 \gamma \%$ confidence interval for $\mu$
		
		
		\section{Three steps of hypothesis testing}
		\begin{enumerate}
			\item Formulate $H_0$ and $H_1$
			\item Do the experiment
			\item Calculate whether results justify rejecting $H_0$
			
			\begin{tabular}{ c | c }
				DO NOT REJECT $H_0$ & REJECT $H_0$ \\
				\hline
				Insufficient evidence & $H_1$ true beyond  \\
				to support $H_1$ & reasonable doubt
			\end{tabular}
		\end{enumerate}
		
		\section{Test statistic}
		Suppose the data are modelled as a realization of random variables
		$X_i$. A test statistic is any sample statistic
		\begin{equation*}
			T=h(X_1,...,X_n)
		\end{equation*}
		whose numerical value is used to decide whether we reject $H_0$
		
		\section{Tail probabilities}
		Give a test statistic $T$, a left tail probability is $P(T \leq t)$ for some $t$.
		A right tail probability is $P(T \geq t)$ \\
		The $p-$value is the probability, given $H_0$ is true, of an event at least as
		extreme as the observations in the direction which provides evidence for $H_1$ \\
		The $p-$value reflects how improbable the observed value $t$ is under $H_0$: small
		$p-$values are bad for the null
		
		\section{Significance level and critical region}
		The significance level $\alpha$ is the largest acceptable probability of
		committing a type I error \\
		Suppose we test $H_0$ against $H_1$ by means of the test statistic $T$. The set
		of values for $T$ for which we reject $H_0$ in favour of $H_1$ is called
		the critical region \\
		Values on the boundary of this region are called critical values
		
		\section{t-test statistic}
		The $t-$test statistic is defined as
		\begin{equation*}
			T = \frac{\bar{X}_n - \mu_0}{S_n / \sqrt{n}} \sim t(n-1)
		\end{equation*}
		
		\section{Normal samples}
		Let $X_1,...,X_n$ be a sample from a $\mathrm{N}(\mu, \sigma^2)$ distribution.
		To test the null hypothesis $H_0: \mu = \mu_0$, we define the $t-$test
		statistic $T$ as
		\begin{equation*}
			T=\frac{\bar{X}_n - \mu_0}{S_n / \sqrt{n}}
		\end{equation*} 
		Then the distribution of this statistic under $H_0: \mu = \mu_0$ is
		\begin{equation*}
			T \sim t(n-1)
		\end{equation*}
		To perform a $t-$test for samples from normal data with unknown variance
		at significance level $\alpha$:
		\begin{enumerate}
			\item Formulate the hypotheses
			\item Compute the value of the $t-$test statistic
			\item Compare this value with the critical values $t_{n-1,\alpha/2}$ or
			$t_{n-1,\alpha}$ depending on two-sided/one-sided test
			\item Decide whether to reject the null hypothesis
		\end{enumerate}
		
		\section{Large samples}
		Let $X_1,...,X_n$ be a sample from an unknown distribution. For large $n$, the
		distribution of the studentized mean can be approximated by the standard
		normal distribution \\
		To perform a $t-$test for samples large samples from non-normal data
		at significance level $\alpha$:
		\begin{enumerate}
			\item Formulate the hypotheses
			\item Compute the value of the $t-$test statistic
			\item Compare this value with the critical values $z_{\alpha/2}$ or
			$z_{\alpha}$ depending on two-sided/one-sided test
			\item Decide whether to reject the null hypothesis
		\end{enumerate}
		
	\end{multicols}
\end{document}







