% Template setup and packages.
\documentclass[10pt,landscape,a4paper]{article}
\usepackage{multicol}
\usepackage{calc}
\usepackage{ifthen}
\usepackage{geometry}

% Custom packages.
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{stix2}
\usepackage{systeme}
\usepackage{graphicx}
\usepackage{float}
\usepackage{physics}
\usepackage{siunitx}
\usepackage{collcell} % loads array
\newcolumntype{M}{>{$} l <{$}}
\newcolumntype{U}{>{$[\collectcell\si} l <{\endcollectcell]$}}

% Hyperref. Remember to change title!
\usepackage{hyperref}
\hypersetup{pdfauthor={Teemu Weckroth},pdftitle={Linear Algebra}}

% Path to graphics folder.
\graphicspath{ {./figures/} }

% Change fonts for v and w.
\DeclareSymbolFont{txletters}{OML}{ntxmi}{m}{it}
\SetSymbolFont{txletters}{bold}{OML}{ntxmi}{b}{it}
\DeclareFontSubstitution{OML}{ntxmi}{m}{it}
\DeclareMathSymbol{v}{\mathalpha}{txletters}{`v}
\DeclareMathSymbol{w}{\mathalpha}{txletters}{`w}

% Must be below font changes to avoid errors.
\usepackage{bm}

% Commands for differentials. Redefines the underdot command!
\renewcommand\d{\mathop{}\!\mathrm{d}}
\newcommand\p{\mathop{}\!\mathrm{\partial}}
%\newcommand\e{\mathrm{e}}

% Commands for the set of real numbers and Lagrangian/Laplace.
\newcommand{\R}{\mathbb{R}}
\newcommand{\La}{\mathscr{L}}

% Unbreakable unit environment.
\newenvironment{absolutelynopagebreak}
{\par\nobreak\vfil\penalty0\vfilneg
	\vtop\bgroup}
{\par\xdef\tpd{\the\prevdepth}\egroup
	\prevdepth=\tpd}

% Shrink bullet points.
\renewcommand\labelitemi{$\vcenter{\hbox{\tiny$\bullet$}}$}

%
\ifthenelse{\lengthtest { \paperwidth = 11in}}
{ \geometry{top=.5in,left=.5in,right=.5in,bottom=.5in} }
{\ifthenelse{ \lengthtest{ \paperwidth = 297mm}}
	{\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
	{\geometry{top=1cm,left=1cm,right=1cm,bottom=1cm} }
}

% Turn off header and footer
\pagestyle{empty}

% Redefine section commands to use less space
\makeatletter
\renewcommand{\section}{\@startsection{section}{1}{0mm}%
	{-1ex plus -.5ex minus -.2ex}%
	{0.5ex plus .2ex}%x
	{\normalfont\large\bfseries}}
\renewcommand{\subsection}{\@startsection{subsection}{2}{0mm}%
	{-1explus -.5ex minus -.2ex}%
	{0.5ex plus .2ex}%
	{\normalfont\normalsize\bfseries}}
\renewcommand{\subsubsection}{\@startsection{subsubsection}{3}{0mm}%
	{-1ex plus -.5ex minus -.2ex}%
	{1ex plus .2ex}%
	{\normalfont\small\bfseries}}
\makeatother

% Define BibTeX command
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
		T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

% Don't print section numbers
\setcounter{secnumdepth}{0}

\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt plus 0.5ex}

\begin{document}
	
	\raggedright
	\footnotesize
	\begin{multicols}{3}
		
		
	% multicol parameters
	% These lengths are set only within the two main columns
	%\setlength{\columnseprule}{0.25pt}
	\setlength{\premulticols}{1pt}
	\setlength{\postmulticols}{1pt}
	\setlength{\multicolsep}{1pt}
	\setlength{\columnsep}{2pt}
	
	\part*{Linear algebra.}
	\begin{center}
		Teemu Weckroth, \today.
	\end{center}
	
	\section*{Invertible Matrix Theorem}
	For $n$ by $n$ matrix $A$ to have an inverse, any (and hence all) must hold
	\begin{enumerate}
		\item $A$ is row equivalent to $I_n$
		\item $A$ has a $n$ pivot positions
		\item $Ax = 0$ only has the trivial solution
		\item Columns of $A$ form a linearly independent set
		\item Linear transformation $x \rightarrowtail Ax$ is one-to-one
		\item $Ax = b$ has a unique solution for each column vector $b \in \mathbb{R}^n$
		\item Columns of $A$ span $\mathbb{R}^n$
		\item Linear transformation $x \rightarrowtail Ax$ is a surjection
		\item There exists $n$ by $n$ matrix $C$ such that $CA = I_n$
		\item There exists $n$ by $n$ matrix $D$ such that $AD = I_n$
		\item Transpose matrix $A^T$ is invertible
		\item Columns of $A$ form a basis for $\mathbb{R}^n$
		\item Column space of $A$ is equal to $\mathbb{R}^n$
		\item Dimension of the column space of $A$ is $n$
		\item Rank of $A$ is $n$
		\item Null space of $A$ is $\{0\}$
		\item Dimension of null space is $0$
		\item $0$ fails to be an eigenvalue of $A$
		\item Determinant of $A$ is not zero
		\item Orthogonal complement of column space of $A$ is $\{0\}$
		\item Orthogonal complement of null space of $A$ is $\mathbb{R}^n$
		\item Row space of $A$ is $\mathbb{R}^n$
		\item Matrix $A$ has $n$ non-zero singular values
	\end{enumerate}
	
	\section*{Echelon form}
	\begin{enumerate}
		\item All non-zero rows are above any row of all zeros
		\item Each leading point (pivot) is in a column to the right of the leading entry in the previous row
		\item All entries below a leading entry are zero
	\end{enumerate}
	
	\section*{Reduced echelon form}
	\begin{enumerate}
		\item The matrix is in echelon form
		\item The pivot of each nonzero row is 1
		\item Each leading 1 is the only nonzero entry in its column
	\end{enumerate}
	
	\section*{Basic, free variables}
	If the augmented matrix is in echelon form:
	\begin{enumerate}
		\item Basic variables of a linear system are those corresponding to pivot columns
		\item Free variables of a linear system are those that have no pivot in the corresponding column
		\item A consistent system is solved by expressing all basic variables in terms of the free variables
	\end{enumerate}
	If there are no free variables, then the system has a unique solution
	
	\section*{Linear combinations}
	Given vectors $v_1, v_2, ..., v_p$ in $\mathbb{R}^n$ and scalars $c_1, c_2, ..., c_p$,
	\begin{equation*}
		y = c_1 v_1 + ... + c_p v_p
	\end{equation*}
	is called a linear combination of $v_1, v_2, ..., v_p$ with weights $c_1, c_2,...c_p$
	
	\section*{Spans}
	\begin{equation*}
		Span\{v_1, v_2, ..., v_p\}
	\end{equation*}
	is the set of all linear combinations of $v_1, v_2, ... v_p$ and consists of all vectors that can be written as
	\begin{equation*}
		x_1 v_1 + x_2 v_2 + ... + x_p v_p
	\end{equation*}
	
	\section*{Matrix-vector product}
	\begin{equation*}
		Ax =
		\begin{bmatrix}
		a_1 & a_2 & ... & a_n\\
		\end{bmatrix}
		\begin{bmatrix}
		x_1 \\ x_2 \\ ... \\ x_n \\
		\end{bmatrix}
		= x_1 a_2 + x_2 a_2 + ... + x_n a_n
	\end{equation*}
	
	\section*{Solutions of matrix equation}
	For $m \times n$ $A$, the following statements are equivalent:
	\begin{enumerate}
		\item For each $b$ in $\mathbb{R}^m$, $Ax = b$ has a solution
		\item Each $b$ in $\mathbb{R}^m$ is a linear combination of the columns of $A$
		\item The columns of $A$ span $\mathbb{R}^m$
		\item Each row in $A$ has a pivot position
	\end{enumerate}
	
	\section*{Linear independence}
	\begin{equation*}
		x_1 v_1 + x_2 v_2 + ... + x_p v_p = 0
	\end{equation*}
	only has the trivial solution \\
	If the set $\{v_1, v_2, ... , v_p\}$ in $\mathbb{R}^n$ contains the zero vector, then the set is linearly dependent
	
	\section*{Linear transformation}
	$T: \mathbb{R}^n \rightarrow \mathbb{R}^m$ is a linear transformation if
	\begin{enumerate}
		\item $T(u + v) = T(u) + T(v)$
		\item $T(cu) = cT(u)$
	\end{enumerate}
	
	If $T$ is a linear transformation, then
	\begin{enumerate}
		\item $T(0) = 0$
		\item $T(c_1 v_1 + ... + c_p v_p) = c_1 T(v_1) + ... + c_p T(v_p)$
	\end{enumerate}
	
	\section*{Linear transformation composition}
	\begin{equation*}
		(S \circ T)(x) = S(T(x))
	\end{equation*}
	
	\section*{Transpose matrix properties}
	\begin{enumerate}
		\item $(A^T)^T = A$
		\item $(A + B)^T = A^T + B^T$
		\item $(r A)^T = r A^T$ for any scalar $r$
		\item $(A B)^T = B^T A^T$
	\end{enumerate}
	
	\section*{Inverse matrices}
	A square $n \times n$ matrix $A$ is invertible if there is an $n \times n$ matrix C such that
	\begin{equation*}
		C A = I_n \textrm{ and } A C = I_n
	\end{equation*}
		
	\section*{System with inverse matrix}
	\begin{equation*}
		Ax = b
	\end{equation*}
	has the unique solution
	\begin{equation*}
		x = A^{-1} b
	\end{equation*}
	
	\section*{Inverse matrix properties}
	\begin{enumerate}
		\item $(A^{-1})^{-1} = A$
		\item $(A B)^{-1} = B^{-1} A^{-1}$
		\item $(A^T)^{-1} = (A^{-1})^T$
	\end{enumerate}
	
	\section*{Subspace}
	A subspace of $\mathbb{R}^n$ is a set $W$ such that:
	\begin{enumerate}
		\item $0$ is in $W$
		\item If $v$ and $u$ in $W$, so is $v + u$
		\item If $v$ in $W$ and $c$ in $\mathbb{R}$, then $cv$ is in $W$
	\end{enumerate}
	The span of a set of vectors $\{v_1, v_2, ... v_n\}$ in $\mathbb{R}^m$ is a subspace of $\mathbb{R}^m$
	
	\section*{Null, column space}
	$Nul A$ of $A$ is the set of all solutions of the homogeneous equation $Ax = 0$. Subspace of $\mathbb{R}^n$ \\
	$Col A$ of $A$ is the span of the columns of $A$. Subspace of $\mathbb{R}^m$
	
	\section*{Basis}
	A basis for subspace $W$ is a set of vectors in $W$ which
	\begin{enumerate}
		\item is linearly independent
		\item spans $W$
	\end{enumerate}
	The pivot columns of $A$ form a basis for the column space of $A$
	
	\section*{Coordinate vector}
	Let $\mathcal{B} = \{b_1, ..., b_p\}$ be a basis for a subspace $W$ of $\mathbb{R}^n$ and $x \in W$. Then there are unique weights $c_1, ... c_p$ such that
	\begin{equation*}
	x = c_1 b_1 + c_2 b_2 + ... + c_p b_p
	\end{equation*}
	Weights $c_1, ... c_p$ form the $\mathcal{B}$-coordinate vector of $x$:
	\begin{equation*}
	[x]_{\mathcal{B}} = \begin{bmatrix}
	c_1 \\ ... \\ c_p
	\end{bmatrix}
	\end{equation*}
	
	\section*{Rank theorem}
	Rank of $A$ is dimension of column space of $A$
	\begin{equation*}
		rank(A) + dim(Nul(A)) = n
	\end{equation*}
	
	\section*{Cofactor expansion}
	Across row $i$:
	\begin{equation*}
		\det A = a_{i 1} C_{i 1} + ... + a_{i n} C_{i n}
	\end{equation*}
	Across column $j$:
	\begin{equation*}
		\det A = a_{1 j} C_{1 j} + ... + a_{n j} C_{n j}
	\end{equation*}
	
	\section*{Row operations, determinants}
	\begin{enumerate}
		\item Interchange two rows: \\ $\det B = - \det A$
		\item Multiply a row by constant $k$: \\ $\det B = k \det A$
		\item Adding multiples of rows: \\ $\det B = \det A$
	\end{enumerate}
	
	\section*{Eigenvalues, eigenvectors}
	Scalar $\lambda$ is an eigenvalue of $A$ is there exists a nonzero eigenvector $x \in \mathbb{R}^n$ for which
	\begin{equation*}
		A x = \lambda x
	\end{equation*}
	
	\section*{Eigenspace $E_{\lambda}$}
	$E_{\lambda} = \mbox{Nul}(A - \lambda I)$ is the set of all solutions of $A x = \lambda x$, forms a subspace of $\mathbb{R}^n$, and consists of
	\begin{enumerate}
		\item all the eigenvectors with eigenvalue $\lambda$
		\item the zero vector
	\end{enumerate}
	
	\section*{Eigenvalue multiplicity}
	Algebraic multiplicity of eigenvalue $\lambda_0$ of $A$ is the number of factors $(\lambda - \lambda_0)$ in the characteristic polynomial.
	\\
	Geometric multiplicity of eigenvalue $\lambda_0$ is the dimension of the corresponding eigenspace $E_{\lambda_{0}}$.
	
	\section*{Similarity}
	$A$ and $B$ similar if there exists $P$ such that $A = P B P^{-1}$.
	\\
	Similar matrices $A$ and $B$ have the same characteristic polynomial and hence the same eigenvalues with the same multiplicities.
	\\
	If $v$ is an eigenvector of $B$ with eigenvalue $\lambda$, then $P v$ is an eigenvector of $A$ with the same eigenvalue $\lambda$.
	
	\section*{Diagonalizable matrix}
	$A$ diagonalizable if it is similar to diagonal matrix $D$ \\
	$D$: eigenvalues of $A$ \\
	$P$: corresponding eigenvectors of $A$ \\
	Diagonalizable if a.m.($\lambda_i$) $=$ g.m.($\lambda_i$) for all $\lambda_i$
	
	\section*{Real matrices, complex eigenvalues}
	Suppose $a, b \in \mathbb{R}^n, b \not= 0$ and
	\begin{equation*}
		A = \begin{bmatrix} a & -b \\ b & a \\ \end{bmatrix}.
	\end{equation*}
	Let $z = a + bi = r e^{i \theta}$. Then
	\begin{equation*}
		A = r \begin{bmatrix}
		\cos \theta & -\sin \theta \\
		\sin \theta & \cos \theta
		\end{bmatrix},
	\end{equation*}
	and the eigenvalues of $A$ are
	\begin{equation*}
		a \pm bi = r e^{\pm i \theta}
	\end{equation*}
	
	\section*{Real 2x2 matrix with non-real eigenvalues}
	Let $A$ be a real $2 \times 2$ matrix with eigenvalues $a \pm bi$, where $b \not= 0$. Then there exists an invertible matrix $P$ such that
	\begin{equation*}
		A = P
		\begin{bmatrix}
		a & -b \\ b & a \\
		\end{bmatrix}
		P^{-1}.
	\end{equation*}
	The matrix $P$ can be constructed as
	\begin{equation*}
		P =
		\begin{bmatrix}
		\textrm{Re} v & \textrm{Im} v \\
		\end{bmatrix},
	\end{equation*}
	where $v$ is an eigenvector associated to eigenvalue $a - bi$
	
	\section*{Differential equations, general}
	Suppose $A$ is an $n \times n$ matrix with $n$ linearly independent eigenvectors $v_j$ and corresponding eigenvalues $\lambda_j$. The general solution to $x^{'} = A x$ is given by
	\begin{equation*}
		x(t) = c_1 v_1 e^{\lambda_1 t} + ... + c_n v_n e^{\lambda_n t}
	\end{equation*}
	
	\section*{Complex eigenvalues}
	Suppose $2 \times 2$ real matrix $A$ has complex eigenvalues $\lambda_{1,2} = a \pm bi$ with corresponding eigenvectors $v_1$ and $v_2$. Two independent, real solutions of the system
	\begin{equation*}
		x' = Ax
	\end{equation*}
	are given by
	\begin{equation*}
		x_1 (t) = \textrm{Re} (v_1 e^{\lambda_1 t}) \textrm{ and }
		x_2 (t) = \textrm{Im} (v_1 e^{\lambda_1 t})
	\end{equation*}
	
	\section*{Different orbit types}
	Let $A$ be a $2 \times 2$ matrix. Consider the system $x' = Ax$, where $A$ has real eigenvalues $\lambda_1$ and $\lambda_2$. The origin is:
	\begin{enumerate}
		\item an attractor/sink if $\lambda_1, \lambda_2 < 0$
		\item a repeller/source if $\lambda_1, \lambda_2 > 0$
		\item a saddle point if $\lambda_1 < 0 < \lambda_2$
	\end{enumerate}
	Let $A$ be a $2 \times 2$ matrix. Consider the system $x' = Ax$, where $A$ has the eigenvalues $\lambda_{1,2} = a \pm bi, b \not= 0$:
	\begin{enumerate}
		\item if $a > 0$, the trajectories spiral away from the origin
		\item if $a < 0$, the trajectories spiral inward towards the origin
		\item if $a = 0$, the trajectories are closed curves around the origin
	\end{enumerate}
	
	\section*{Orthogonal complement}
	Given a subspace $W$ of $\mathbb{R}^n$, the orthogonal complement of $W$ is the set of all vectors in $\mathbb{R}^n$ that are orthogonal to all vectors in $W$: $W^T$
	\begin{enumerate}
		\item The orthogonal complement of $W$ is a subspace of $\mathbb{R}^n$
		\item Let $W \subset \mathbb{R}^n$ be a subset, then $(W^T)^T = W$
	\end{enumerate}
	
	\section*{Fundamental matrix spaces}
	(Col$A$)$^{\perp}$ $=$ Nul$A^T$ and (Col$A^T$)$^{\perp}$ $=$ Nul$A$
	\\
	The subspace Col$A^T$ is the Row Space of $A$
	
	\section*{Orthogonal, orthonormal sets}
	A set of vectors $\{u_1, ..., u_p\}$ in $\mathbb{R}^n$ is called an
	\begin{enumerate}
		\item orthogonal set if $u_i \cdot u_j = 0$ for each pair $i \not= j$
		\item orthonormal set if $u_i \cdot u_j =$
		$\begin{cases}
			0, & \mbox{if } i \not= j\\
			1, & \mbox{if } i = j\\
		\end{cases}$
	\end{enumerate}
	Any orthogonal set which doesn't contain the zero vector is linearly independent
	
	\section*{Coordinates with respect to orthogonal basis}
	Let $\mathcal{B} = \{v_1, ..., v_p\}$ be an orthogonal basis for a subspace $W$ in $\mathbb{R}^n$. Then any vector $y$ in $W$ has the following decomposition w.r.t. $\mathcal{B}$:
	\begin{equation*}
		y = \frac{y \cdot v_1}{v_1 \cdot v_1} v_1 + ... + \frac{y \cdot v_p}{v_p \cdot v_p} v_p
	\end{equation*}
	Denominators disappear for an orthonormal basis
	
	\section*{Orthonormal columns}
	An $m \times n$ matrix $U$ has orthonormal columns iff
	\begin{equation*}
		U^T U = I
	\end{equation*}
	Let $U$ be an $m \times n$ matrix with orthonormal columns and $x, y$ vectors in $\mathbb{R}^n$:
	\begin{enumerate}
		\item $(Ux) \cdot (Uy) = x \cdot y$
		\item $|| Ux || = || x ||$
		\item $(Ux) \perp (Uy)$ iff $x \perp y$
	\end{enumerate}
	$U^{-1} = U^T$
	
	\section*{Projection}
	\begin{equation*}
		\hat{y} = \textrm{proj}_{L} (y) = \frac{y \cdot u}{u \cdot u} u
	\end{equation*}
	
	\section*{Orthogonal decomposition theorem}
	$y = \hat{y} + z$ where $\hat{y} = $ proj$_W (y)$ and $z \in W^{\perp}$
	
	\section*{Subspaces with an orthonormal basis}
	Let $\{u_1, ..., u_p\}$ be an orthonormal basis of $W \subset \mathbb{R}^n$:
	\begin{enumerate}
		\item proj$_W (y) = U U^T y$ with $U = [u_1 u_2 ... u_p]$
		\item if $P = U U^T$ is the standard matrix of proj$_W$, then $P^2 = P = P^T$
	\end{enumerate}
	
	\section*{Best approximation}
	\begin{equation*}
	||y - \hat{y}|| \leq ||y - v|| \textrm{ for all } v \in W
	\end{equation*}
	
	\section*{Least-squares solution}
	\begin{equation*}
	||b - A\hat{x}|| \leq ||b - Ax|| \textrm{ for all } x \in R^n
	\end{equation*}
	Distance $|| b - A \hat{x}||$ is the least-squares error
	
	\section*{Spectral theorem}
	An $n \times n$ symmetric matrix $A$ has the following properties
	\begin{enumerate}
		\item $A$ has $n$ real eigenvalues, counting multiplicities
		\item For each eigenvalue, the geometric multiplicity is equal to the algebraic multiplicity
		\item The eigenspaces are mutually orthogonal, in the sense that eigenvectors corresponding to different eigenvalues are orthogonal
		\item $A$ is orthogonally diagonalizable
	\end{enumerate}
	
	
	
\end{multicols}
\end{document}










